[
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html",
    "title": "A tidystats example",
    "section": "",
    "text": "In this post, I will illustrate how to use tidystats by analyzing data of the Many Labs 1 (Klein et al. 2014) replication of Lorge & Curtiss (1936)."
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html#analyzing-the-data",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html#analyzing-the-data",
    "title": "A tidystats example",
    "section": "Analyzing the data",
    "text": "Analyzing the data\nLorge and Curtiss (1936) examined how a quotation is perceived when it is attributed to a liked or disliked individual. In one condition the quotation was attributed to Thomas Jefferson and in the other it was attributed to Vladimir Lenin. They found that people agree more with the quotation when the quotation was attributed to Jefferson than Lenin. In the Many Labs replication study, the quotation was attributed to either George Washington, the liked individual, or Osama Bin Laden, the disliked individual. The also used a different quotation, which was:\n\nI have sworn to only live free, even if I find bitter the taste of death.\n\nWe are again interested in testing whether the source of the quotation affects how it is evaluated. The evaluation was assessed on a 9-point Likert scale ranging from 1 (strongly agree) to 9 (strongly disagree). I reverse coded this in the data that we’ll use. You can follow along by copy-pasting the code from this example.\nBefore getting into how tidystats should be used, let’s first simply analyze the data. I have designed tidystats to be minimally invasive. In other words, to use tidystats, you do not need to substantially change your data analysis workflow.\nWe’ll start with a basic setup where we load some packages and the data.\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(tidystats)\n\n# Load example data\ndata <- data(quote_source)\n\n# Create color variables for the plots\nbackground_color <- \"#1a1a1a\"\ngreen <- \"#00B88D\"\nyellow <- \"#f39c12\"\n\n\n\n\n\nOur main effect of interest is the difference in responses to the quote between the two conditions. Here I visualize this different with a violin plot.\n\n\nCode\nggplot(quote_source, aes(x = source, y = response, fill = source)) +\n  geom_violin(alpha = 0.95, color = \"transparent\") +\n  stat_summary(fun = \"mean\", geom = \"point\", color = background_color) +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  labs(x = \"Quote source\", y = \"Quote agreement\") +\n  scale_fill_manual(values = c(green, yellow)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\n\nThis looks like the effect is in the expected direction. Participants agreed more with the quotation when they believed the quote to be from George Washington compared to Osama Bin Laden.\nRegarding descriptives, tidystats comes with its own functions to calculate descriptives. One of them is the describe_data function, inspired by the describe function from the psych package. You can use it together with group_by from the dplyr package to calculate a set of descriptives for multiple groups.\n\n\nCode\nquote_source %>%\n  group_by(source) %>%\n  describe_data(response)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nsource\nmissing\nN\nM\nSD\nSE\nmin\nmax\nrange\nmedian\nmode\nskew\nkurtosis\n\n\n\n\nresponse\nBin Laden\n18\n3083\n5.2\n2.1\n0.04\n1\n9\n8\n5\n5\n-0.08\n2.6\n\n\nresponse\nWashington\n0\n3242\n5.9\n2.2\n0.04\n1\n9\n8\n6\n5\n-0.23\n2.2\n\n\n\n\n\nTo test whether the difference in agreement between the two sources is statistically significant, we perform a t-test. Normally, we would just run the t-test like so:\n\n\nCode\nt.test(response ~ source, data = quote_source)\n\n\nHowever, since we want to use tidystats to later save the statistics from this test, we will store the output of the t-test in a variable. This, and the final section of R code, will be the only thing you need to change in order to incorporate tidystats in your workflow.\nOnce you’ve stored the result of the t-test in a variable, you can look at the output by sending it the console, which will print the output.\n\n\nCode\nmain_test <- t.test(response ~ source, data = quote_source)\nmain_test\n\n\n\n\n\n\n\nt\np\ndf\ndifference\n95% CI lower\n95% CI upper\n\n\n\n\n-12.80\n4.6e-37\n6322.72\n-0.70\n-0.80\n-0.59\n\n\n\n\n\nThis shows us that there is a statistically significant effect of the quote source, consistent with the hypothesis.\nNext, let’s run some additional analyses. One thing we can test is whether the effect is stronger in the US compared to non-US countries. To test this, we perform a regression analysis. Here we also store the result in a variable, but this is actually quite common in regression analyses because you want to apply the summary function to this variable in order to obtain the inferential statistics.\n\n\nCode\nus_moderation_test <- lm(response ~ source * us_or_international, \n  data = quote_source)\nsummary(us_moderation_test)\n\n\n\n\n\n\n\nTerm\nb\nSE\nt\np\n\n\n\n\n(Intercept)\n5.249\n0.085\n61.83\n0.00000\n\n\nsourceWashington\n0.405\n0.117\n3.46\n0.00055\n\n\nus_or_internationalUS\n-0.021\n0.095\n-0.22\n0.82589\n\n\nsourceWashington:us_or_internationalUS\n0.372\n0.132\n2.81\n0.00497\n\n\n\n\n\nThere appears to be a significant interaction. Let’s inspect the interaction with a graph:\n\n\nCode\nggplot(quote_source, aes(x = us_or_international, y = response, \n    fill = source)) +\n  geom_violin(alpha = .95, color = \"transparent\") +\n  stat_summary(fun.data = \"mean_cl_boot\", position = position_dodge(.9), \n    color = background_color) +\n  scale_fill_manual(values = c(green, yellow)) +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  labs(x = \"Region\", y = \"Quote agreement\", fill = \"Source\")\n\n\n\n\n\n\n\n\n\nWe see that the effect of the source appears to be larger in the US. Given that the positive source was George Washington, this makes sense.\nLet’s do one more analysis to see whether the effect is stronger in a lab setting compared to an online setting.\n\n\nCode\nlab_moderation_test <- lm(response ~ source * lab_or_online, data = quote_source)\nsummary(lab_moderation_test)\n\n\n\n\n\n\n\nTerm\nb\nSE\nt\np\n\n\n\n\n(Intercept)\n5.197\n0.057\n91.59\n0.0e+00\n\n\nsourceWashington\n0.686\n0.079\n8.68\n5.1e-18\n\n\nlab_or_onlineonline\n0.066\n0.078\n0.85\n3.9e-01\n\n\nsourceWashington:lab_or_onlineonline\n0.017\n0.109\n0.16\n8.7e-01\n\n\n\n\n\nWe see no significant interaction in this case. This means we do not find evidence that running the study in an online setting significantly weakens the effect; good to know!"
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html#applying-tidystats",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html#applying-tidystats",
    "title": "A tidystats example",
    "section": "Applying tidystats",
    "text": "Applying tidystats\nNow let’s get to tidystats. We have three analyses we want to save: a t-test and two regression analyses. We stored each of these analyses in separate variables, called main_test, us_moderation_test, and lab_moderation_test.\nThe main idea is that we will add these three three variables to a list and then save the list as a file on our computer. You create an empty list using the list function. Once you have an empty list, you can add statistics to this list using the add_stats function. add_stats accepts a list as its first argument, followed by a variable containing a statistics model. In our case, this means we need to use the add_stats function three times, as we have three different analyses we want to save. Since this can get pretty repetitive, we will use the piping operator to pipe the three steps together and save some typing.\nBefore we do so, however, note that we can take this opportunity to add some meta-information to each test. For the sake of this example, let’s say that the t-test was our primary test. We also had a suspicion that the location (US vs. international) would matter, but it wasn’t our main interest. Nevertheless, we preregistered these two analyses. During data analysis, we figured that it might also matter whether the study was conducted in the lab or online, so we tested it. This means that this is an exploratory analysis. With add_stats, we can add this information when we add the test to our empty list.\nIn the end, the code looks like this:\n\n\nCode\nresults <- list()\n\nresults <- results %>%\n  add_stats(main_test, type = \"primary\", preregistered = TRUE) %>%\n  add_stats(us_moderation_test, type = \"secondary\", preregistered = TRUE) %>%\n  add_stats(lab_moderation_test, type = \"secondary\", preregistered = FALSE)\n\n\nI recommend to do this at the end of the data analysis script in a section called ‘tidystats’. This confines most of the tidystats code to a single section, keeping it organized, and it will keep most of your script readable to those unfamiliar with tidystats.\nAfter all the analyses are added to the list, the list can be saved as a .json file to your computer’s disk. This is done with the write_stats function. The function requires the list as its first argument, followed by a file path. I’m a big fan of using RStudio Project files so that you can define relative file paths. In this case, I create the .json file in the ‘Data’ folder of my project folder.\n\n\nCode\nwrite_stats(results, \"Data/tidystats-example.json\")\n\n\nIf you want to see what this file looks like, you can inspect it here. Open the file in a text editor to see how the statistics are structured. As you will see, it is not easy for our human eyes to quickly see the results, but it’s easy for computers.\nOnce you’ve saved the file, you can share the file with others or use it to report report the results in your manuscript. Below I show a video of how you can upload the file in the tidystats Word add-in and report the results.\n\n\n\nThat marks the end of this tidystats example. If you have any questions, please check out the tidystats website or contact me via Twitter.\nThis post was last updated on 2022-04-07."
  },
  {
    "objectID": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html",
    "href": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html",
    "title": "Useful power analysis papers",
    "section": "",
    "text": "In this post I list papers that are useful for understanding power analyses. A curious thing happened in the field of social psychology: Social psychologists finally realized that statistical power is important. Unfortunately, they then skipped the step of figuring out how to do them correctly. Here I list some papers on power analyses that I hope help in improving the way we do them."
  },
  {
    "objectID": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html#papers",
    "href": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html#papers",
    "title": "Useful power analysis papers",
    "section": "Papers",
    "text": "Papers\n\nMaxwell, S. E. (2004). The persistence of underpowered studies in psychological research: Causes, consequences, and remedies. Psychological Methods, 9(2), 147–163. https://doi.org/10.1037/1082-989X.9.2.147\n\nThis paper is quite amazing. It covers almost everything you need to be aware of when it comes to the state of our field, including how badly powered most studies are, how this affects the interpretibility of inconsistencies in the literature, the need for multi-site projects, and so on!\nMost importantly, it is about the problem of multiple statistical tests. Many power analyses that I see in the literature only power for one analysis, even though a paper usually contains many more. If you want a shot at all of those analyses being able to show something, you need to power for it all.\n\nBlake, K. R., & Gangestad, S. (2020). On attenuated interactions, measurement error, and statistical power: Guidelines for social and personality psychologists. Personality and Social Psychology Bulletin. https://doi.org/10.1177/0146167220913363\n\nThis is a great paper on a common pitfall in power analyses for attentuated interaction tests. Attenuated interactions are interactions where you expect a predictor to have an effect in one condition, but not another. They show that in a 2 x 2 design, you should use a fourfold of the sample size that is needed to show the interaction effect. Of course, this only applies when you predict a perfect attenuated interaction (a complete absence of the effect in the other condition, rather than a diminished one) and that you do not have any measurement error.\nThis post was last updated on 2022-04-07."
  },
  {
    "objectID": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "href": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "title": "The right order of Method sections",
    "section": "",
    "text": "I think the proper order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants\n\nTwo things are notable here. One, there’s a Data Analysis section. Two, the Participants section is all the way at the end. Here I anticipate that your reaction will be that this is crazy, because that’s not how we do things. But that’s not a good enough reason of course. We should be thinking about whether the order makes sense in terms of whether the content of each section logically follows from each other. For some sections you first need to know information from the other sections in order for your decisions to make sense. Putting the Participants all the way at the beginning doesn’t make sense, and the reason for that is the power analysis.\nNow that power analyses are getting more popular, psychologists have to try and make them fit in their Method section. But rather than thinking about what actually goes into a power analysis, and how to present that information to the reader, they generally stick to the format they’re used to. Or perhaps it’s because they misunderstand how a power analysis works, thinking that you only have 1 power analysis per study, so you should present it together with the Design of the study. Since the Design and Participants are sometimes combined, I can see how this might be the case. That still doesn’t make sense, though, and to understand that, we need to understand power analyses.\nWhat goes into a power analysis? A power analysis consists of setting a few parameters, such as the effect size, alpha, and beta. The alpha and beta parameters are pretty constant across different power analyses, but the effect size isn’t. The effect size depends on the exact analysis you want to power for. A t-test is usually done with a Cohen’s d in mind, while a correlation test is done with a correlation in mind. With more sophisticated analyses, such as repeated measures analyses, you need to set additional parameters (e.g., the correlation between repeated measures). This means that your power analysis is dependent on the exact analysis you will do. Actually, a power analysis is always about a specific analysis, so by that logic alone, you should first present which analysis you will do. Not only that, but you also need to power for all analyses you do, not just 1. In other words, a power analysis is something that is tied to a statistical test, and not to the design of a study (which would mean you only need 1 power analysis per study). The result is obvious: you first need to discuss the analyses you want to run before you can talk about power. This means you need a Data Analysis section in your Method section. Here you can elaborate on the analyses you will run, which analyses are the primary ones that you want to power for, and perhaps elaborate on some secondary or exploratory analyses that you won’t power for. You can also use this section to then present the power analysis. After that you get your needed sample size and you can start to explain how you obtained that sample size (i.e., the Participants section).\nWhat do you need to know in order to understand the Data Analysis section? That would be the Design and Materials. You need to know about the design to know whether it is, for example, a between-subjects design or a within-subjects design. You also need to know what the independent variables and dependent variables are. More specifically, you want to know how they are measured. How many levels are there in the independent variables? Is the outcome measure categorical or continuous? These are some of the properties of the measures that determine the appropriate analysis technique. This, in turn, means the Design section and the Materials section need to come before the Data Analysis section.\nPutting all of this together, I think it makes the most sense to begin with the Design, followed by the Procedure and Materials (possibly combined). This should be followed by a Data Analysis section that includes the analyses and associated power analysis (for all primary analyses, at least). Once these aspects are known, it makes sense to end, rather than start, with the Participants section. So the right order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "title": "Why divide by N - 1 to calculate the variance of a sample?",
    "section": "",
    "text": "In a recent tweet I asked the question why we use \\(n - 1\\) to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. The response consists of some statistical jargon that confuses me more, rather than less. Some of the responses were very useful, though, so I recommend checking out the replies to the tweet. Based on some of the responses I received, I will try to describe my favorite way of looking at the issue.\nIf you want to follow along in R, you can copy the code from each code section; beginning with some setup code."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "title": "Why divide by N - 1 to calculate the variance of a sample?",
    "section": "The Formula",
    "text": "The Formula\nThe formula for calculating the variance is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\]\nThe variance is a measure of the dispersion around the mean, and in that sense this formula makes sense. We calculate all the deviations from the mean (\\(x_i - \\overline{x}\\)), square them (for reasons I might go into in a different post) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.\nHowever, this is the formula for the population variance. The formula for calculating the variance of a sample is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n - 1}\\]\nWhy do we divide by n - 1?\nIf you Google this question, you will get a variety of answers. You might find a mathematical proof of why it needs to be \\(n - 1\\) or something about degrees of freedom. These kinds of answers don’t work for me. I trust them to be correct, but it doesn’t produce any insight. It does not actually help me understand 1) the problem and 2) why the solution is the solution that it is. So, below I am going to try to figure it out in a way that actually makes conceptual and intuitive sense (to me)."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "title": "Why divide by N - 1 to calculate the variance of a sample?",
    "section": "The Problem",
    "text": "The Problem\nThe problem with using the population variance formula to calculate the variance of a sample is that it is biased. It is biased in that it produces an underestimation of the true variance. Let’s demonstrate that with some simulated data.\nWe simulate a population of 1000 data points from a uniform distribution with a range from 1 to 10. Below I show the histogram that represents our population.\n\n\nCode\n# Set the seed for reproducibility\nset.seed(1212)\n\n# Create a population consisting of values ranging from 1 to 10\npopulation <- sample(1:10, size = 1000, replace = TRUE)\n\n# Get the mean and population variance\nmu <- sum(population)/length(population)\nsigma <- my_var(population, population = TRUE)\n\n# Visualize the population\nggplot(tibble(population = population), mapping = aes(x = population)) +\n  geom_bar(alpha = .85) +\n  labs(x = \"x\", y = \"n\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\nThe variance is 8.755775. Note that this is our population variance (often denoted as \\(\\sigma^2\\)). We want to estimate this value using samples drawn from our population, so let’s do that.\n\n\nCode\n# Draw a single sample from the population\nsample <- sample(population, size = 5)\n\n\nTo start, we can draw a single sample of size 5. Say we do that and get the following values: 7, 6, 3, 5, 5. We can then calculate the variance in two ways, using division by \\(n\\) and division by \\(n - 1\\). In the former case, this will result in 1.76 and in the latter case it results in 2.2.\nNow let’s do that many many times. Below I show the results of draws from our population. I simulated drawing samples of size 2 to 10, each 1000 different times. I then plotted for each sample size the average biased variance (dividing by \\(n\\); left) and the average unbiased variance (dividing by \\(n - 1\\); right).\n\n\nCode\n# Create an empty data frame with the simulation parameters\nsamples <- crossing(\n    n = 2:20,\n    i = 1:1000\n  )\n\n# Calculate the mean, sample variance, and population variance \n# for each combination of n and i\nsamples <- samples %>%\n  rowwise() %>%\n  mutate(\n    var_unbiased = my_var(sample(population, n), population = FALSE),\n    var_biased = my_var(sample(population, n), population = TRUE)\n  )\n\np1 <- ggplot(samples, aes(x = n, y = var_biased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2)) +\n  ggtitle(\"Variance with division by n\")\n\np2 <- ggplot(samples, aes(x = n, y = var_unbiased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2)) +\n  ggtitle(\"Variance with division by n - 1\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nWe see that the biased measure of variance is indeed biased. The average variance is lower than the true variance (indicated by the dashed line), for each sample size. We also see that the unbiased variance is indeed unbiased. On average, the sample variance matches that of the population variance.\nThe results of using the biased measure of variance reveals several clues for understanding the solution to the bias. We see that the amount of bias is larger when the sample size of the samples is smaller. So the solution should be a function of sample size, such that the required correction will be smaller as the sample size increases. We also see that that the bias at \\(n = 2\\) is half that of the true variance, \\(\\frac23\\) at \\(n = 3\\), \\(\\frac34\\) at \\(n = 4\\), and so on. Interesting.\nBut before we go into the solution, we still need to figure out what exactly causes the bias.\nIdeally we would estimate the variance of the sample by subtracting each value from the population mean. However, since we don’t know what the population mean is, we use the next best thing—the sample mean. This is where the bias comes in. When you use the sample mean, you’re guaranteed that the mean lies somewhere within the range of your data points. In fact, the mean of a sample minimizes the sum of squared deviations from the mean. This means that the sum of deviations from the sample mean is always smaller than the sum of deviations from the population mean. The only exception to that is when the sample mean happens to be the population mean.\nLet’s illustrate this with a few graphs. Below are two graphs. In each graph I show 10 data points that represent our population. I also highlight two data points from this population, which represents our sample. In the left graph I show the deviations from the sample mean and in the right graph the deviations from the population mean.\n\n\nCode\nx <- c(1, 2, 4, 4, 4, 6, 8, 9, 10, 10)\n\nsample1 <- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 0, 0, 0, 0, 0, 1, 0, 0, 1),\n    mean = mean(c(8, 10)),\n    mu = mean(x)\n  ) %>%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\np1 <- ggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  coord_flip() +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(axis.text.y = element_blank()) +\n  labs(x = \"\", y = \"\") +\n  ggtitle(\"Deviations from the sample mean\")\n\np2 <- ggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  coord_flip() +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(axis.text.y = element_blank()) +\n  labs(x = \"\", y = \"\") +\n  ggtitle(\"Deviations from the population mean\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nWe see that in the left graph the sum of squared deviations is much smaller than in the right graph. The sum is \\((8 - 9)² + (10 - 9)² = 2\\) in the left graph and in the right graph it’s \\((8 - 5.8)² + (10 - 5.8)² = 22.48\\). The sum is smaller when using the sample mean compared to using the population mean.\nThis is true for any sample you draw from the population (again, except when the sample mean happens to be the same as the population mean). Let’s look at one more draw where the sample mean is closer to the population mean.\n\n\nCode\nsample2 <- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 0),\n    mean = mean(c(2, 10)),\n    mu = mean(x)\n  ) %>%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\np1 <- ggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  coord_flip() +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(axis.text.y = element_blank()) +\n  labs(x = \"\", y = \"\") +\n  ggtitle(\"Deviations from the sample mean\")\n  \np2 <- ggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  coord_flip() +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(axis.text.y = element_blank()) +\n  labs(x = \"\", y = \"\") +\n  ggtitle(\"Deviations from the population mean\")\n\np1 + p2\n\n\n\n\n\nHere the sum in the left graph is \\((2 - 6)² + (10 - 6)² = 32\\) and the sum in the right graph is \\((2 - 5.8)² + (10 - 5.8)² = 32.08\\). The difference is small now, but using the sample mean still results in a smaller sum compared to using the population mean.\nIn short, the source of the bias comes from using the sample mean instead of the population mean. The sample mean is always guaranteed to be in the middle of the observed data, thereby reducing the variance, and creating an underestimation."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "title": "Why divide by N - 1 to calculate the variance of a sample?",
    "section": "The Solution",
    "text": "The Solution\nNow that we know that the bias is caused by using the sample mean, we can figure out how to solve the problem.\nLooking at the previous graphs, we see that if the sample mean is far from the population mean, the sample variance is smaller and the bias is large. If the sample mean is close to the population mean, the sample variance is larger and the bias is small. So, the more the sample mean moves around the population mean, the greater the bias.\nIn other words, besides the variance of the data points around the sample mean, there is also the variance of the sample mean around the population mean. We need both variances in order to accurately estimate the population variance.\nThe population variance is thus the sum of two variances:\n\\[\\sigma^2_{sample} + \\sigma^2_{\\vphantom{sample}mean} = \\sigma^2_{population}\\] Let’s confirm that this is true. For that we need to know how to calculate the variance of the sample mean around the population mean. This is relatively simple; it’s the variance of the population divided by n (\\(\\frac{\\sigma^2}n\\)). This makes sense because the greater the variance in the population, the more the mean can jump around, but the more data you sample, the closer you get to the population mean.\nNow that we can calculate both the variance of the sample and the variance of the sample mean, we can check whether adding them together results in the population variance.\nBelow I show a graph in which I again sampled from our population with varying sample sizes. For each sample, I calculated the sample variance (the biased one) and the variance of the mean of that sample (\\(\\frac{\\sigma^2}n\\))1. I did this 1000 times per sample size, took the average of each and put them on top of each other. I also added a dashed line to indicate the variance of the population, which is the benchmark we’re trying to reach.\n\n\nCode\n# Calculate the variance sources per sample size\nvariance_sources <- samples %>%\n  mutate(var_mean = var_unbiased / n) %>%\n  group_by(n) %>%\n  summarize(\n    var_biased = mean(var_biased),\n    var_unbiased = mean(var_unbiased),\n    var_mean = mean(var_mean)\n  ) %>%\n  pivot_longer(cols = c(var_biased, var_mean), names_to = \"variance_source\", \n    values_to = \"variance\") %>%\n  mutate(variance_source = recode(variance_source, \"var_biased\" = \n      \"sample\", \"var_mean\" = \"sample mean\"))\n\nggplot(variance_sources, aes(x = n, fill = variance_source, y = variance)) +\n  geom_col() +\n  geom_hline(yintercept = sigma, linetype = \"dashed\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\", fill = \"Variance source:\")\n\n\n\n\n\nIndeed, we see that the variance of the sample and the variance of the mean of the sample together form the population variance."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "title": "Why divide by N - 1 to calculate the variance of a sample?",
    "section": "The Math",
    "text": "The Math\nNow that we know that the variance of the population consists of the variance of the sample and the variance of the sample mean, we can figure out the correction factor we need to apply to make the biased variance measure unbiased.\nPreviously, we found an interesting pattern in the simulated samples, which is also visible in the previous figure. We saw that at a sample size \\(n=2\\), the (biased) sample variance appears to be half that of the (unbiased) population variance. At sample size \\(n=3\\), it’s \\(\\frac23\\). At sample size \\(n=4\\), it’s \\(\\frac34\\), and so on.\nThis means that we can fix the biased variance measure by multiplying it with \\(\\frac{n}{(n-1)}\\). At sample size \\(n = 2\\), this would mean we multiply the biased variance by \\(\\frac21 = 2\\). For sample size \\(n=3\\), \\(\\frac32 = 1.5\\). At sample size \\(n=4\\), \\(\\frac43 = 1 \\frac13\\), resulting in the unbiased variance.\nIn other words, to unbias the biased variance measure, we multiply it by a correction factor of \\(\\frac{n}{(n-1)}\\). But where does this correction factor come from?\nWell, because the sample variance misses the variance of the sample mean, we can expect that the variance of the sample is biased by an amount equal to the variance of the population minus the variance of the sample mean. In other words:\n\\[\\sigma^2 - \\frac{\\sigma^2}n\\]\nRewriting this 2, produces:\n\\[\\sigma^2\\cdot\\frac{n - 1}n\\] The variance of a sample will be biased by an amount equal to \\(\\frac{n - 1}n\\). To correct that bias we should multiply the sample variance by the inverse of this bias: \\(\\frac{n}{n-1}\\) 3. This is also called Bessel’s correction.\nSo, an unbiased measure of our sample variance is the biased sample variance times the correction factor:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\cdot{\\frac n{n-1}}\\] Because the n in the denominator of the left term (the biased variance formula) cancels out the n in the numerator of the right term (the bias correction), the formula can be rewritten as:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n-1}\\]"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "title": "Why divide by N - 1 to calculate the variance of a sample?",
    "section": "Summary",
    "text": "Summary\nWe calculate the variance of a sample by summing the squared deviations of each data point from the sample mean and dividing it by \\(n - 1\\). The \\(n - 1\\) actually comes from a correction factor \\(\\frac n{n-1}\\) that is needed to correct for a bias caused by taking the deviations from the sample mean rather than the population mean. Taking the deviations from the sample mean only constitutes the variance around the sample mean, but ignores the variation of the sample mean around the population mean, producing an underestimation equal to the size of the variance of the sample mean: \\(\\frac{\\sigma^2}{n}\\). The correction factor corrects for this underestimation, producing an unbiased estimate of the population variance.\nThis post was last updated on 2022-04-07."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "title": "Understanding Regression (Part 1)",
    "section": "",
    "text": "Statistical regression techniques are an important concept in data analysis. As a social scientist, I use it to test hypotheses by comparing differences between groups or testing relationships between variables. While it is easy to run regression analyses in a variety of software packages, like SPSS or R, it often remains a black box that is not well understood. I, in fact, do not believe I actually understand regression. Not fully understanding the mechanics of regression could be okay, though. After all, you also don’t need to know exactly how car engines work in order to drive a car. However, I think many users of regression have isolated themselves too much from the mechanics of regression. This may be the source of some errors, such as applying regression to data that is not suitable for the regression method. If you’re using regression to try and make inferences about the world, it’s probably also a good idea to feel like you know what you’re doing.\nSo, there are some reasons to figure out regression. This post is Part 1 of a series of blog posts called ‘Understanding Regression’ in which I try to figure it out.\nFeel free to follow me along by copy-pasting the code from each step. I’ll add a note in case you need to slightly adjust the code for it to work on your computer."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "title": "Understanding Regression (Part 1)",
    "section": "Setup",
    "text": "Setup\nTo figure out regression, we need data. We could make up some data on the spot, but I’d rather use data that is a bit more meaningful (to me, anyway). Since I’m a big Pokémon fan, I’ll use a data set containing Pokémon statistics.\nIn case you’re following along, start by loading some packages and reading in the data. In the code section below I use the here package to read in the data, but I recommend that you simply specify the path to the file. After that, I subset the data to make the data a bit more manageable and define a custom mode function because R does not have one (and I need it later).\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\n\n# Read in Pokémon data\npokemon <- read_csv(here(\"data\", \"pokemon.csv\"))\n\n# Create a subset with only the first 25 Pokémon\npokemon25 <- filter(pokemon, pokedex <= 25)\n\n# Load a custom function to calculate the mode\nmode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n\nLet’s take a look at several attributes of some Pokémon to see what they’re about:\n\n\nCode\npokemon25 %>%\n  filter(pokedex <= 10) %>%\n  select(name, type_primary, type_secondary, height, weight, \n    evolution) %>%\n  kable(digits = 2)\n\n\n\n\n\nname\ntype_primary\ntype_secondary\nheight\nweight\nevolution\n\n\n\n\nBulbasaur\nGrass\nPoison\n0.7\n6.9\n0\n\n\nIvysaur\nGrass\nPoison\n1.0\n13.0\n1\n\n\nVenusaur\nGrass\nPoison\n2.0\n100.0\n2\n\n\nCharmander\nFire\nNA\n0.6\n8.5\n0\n\n\nCharmeleon\nFire\nNA\n1.1\n19.0\n1\n\n\nCharizard\nFire\nFlying\n1.7\n90.5\n2\n\n\nSquirtle\nWater\nNA\n0.5\n9.0\n0\n\n\nWartortle\nWater\nNA\n1.0\n22.5\n1\n\n\nBlastoise\nWater\nNA\n1.6\n85.5\n2\n\n\nCaterpie\nBug\nNA\n0.3\n2.9\n0\n\n\n\n\n\nPokémon have different types (e.g., grass, fire, water), a height, some weight, and they are of a particular evolutionary stage (0, 1, or 2). This last variable refers to a Pokémon’s ability to evolve and when they do, they tend to become bigger and more powerful.\nLet’s say that we are interested in understanding the weight of different Pokémon. Below I have plotted the weight of the first 25 Pokémon, from Bulbasaur to Pikachu.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nWe see that the lightest Pokémon is Pidgey, with a weight of 1.8 kg. The heaviest Pokémon is Venusaur, with a weight of 100 kg. The average weight is 26.144 kg."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "title": "Understanding Regression (Part 1)",
    "section": "The simplest model",
    "text": "The simplest model\nIn order to understand the weights of different Pokémon, we need to come up with a statistical model. In a way, this can be considered a description problem. How can we best describe the different weights that we have observed? The simplest description is a single number. We can say that all Pokémon have a weight of say… 6 kg. In other words:\n\nweight = 6 kg\n\nOf course, this is just one among many possible models. Below I plot three different models, including our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  geom_abline(intercept = 6, slope = 0, linetype = 2) +\n  geom_abline(intercept = 40, slope = 0, linetype = 2) +\n  geom_abline(intercept = 75, slope = 0, linetype = 2) +\n  annotate(\"text\", x = 28, y = 6.5, label = \"weight = 6 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 40.5, label = \"weight = 40 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 75.5, label = \"weight = 75 kg\", size = 3.5) +\n  coord_cartesian(xlim = c(1, 25), clip = \"off\") +\n  theme(plot.margin = unit(c(1, 6, 1, 1), \"lines\"))\n\n\n\n\n\n\n\n\n\nWhile a model like weight = 6 kg is a valid model, it is not a very good model. In fact, it only perfectly describes Pikachu’s weight and inaccurately describes the weight of the remaining 24 Pokémon. The other models, such as weight = 40 kg might be even worse; they do not even describe a single Pokémon’s weight correctly, although they do get closer to some of the heavier Pokémon. How do we decide which model is the better model? In order to answer that question, we need to consider the model’s error."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#error",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#error",
    "title": "Understanding Regression (Part 1)",
    "section": "Error",
    "text": "Error\nThe error of a model is the degree to which the model inaccurately describes the data. There are several ways to calculate that error and we will cover three different definitions.\nThe first definition of error is simply the sum of times that the model inaccurately describes the data. For each observation we check whether the model correctly describes it or not. We then sum the number of misses and consider that the amount of error for that model. With our weight = 6 kg the answer is 24; out of the 25 Pokémon only Pikachu has a weight of 6, which means the model is correct once and wrong 24 times.\nWe can now compare different models to one another by calculating the error for a range of models. Below I plot the number of errors for 100 different models, starting with the model weight = 1 kg, up to weight = 10 kg, in steps of 0.1. Ideally we would test more models (up to the heaviest Pokémon we know of), but for the sake of visualizing the result, I decided to only plot a small subset of models.\n\n\nCode\nexpand_grid(\n    model = seq(from = 1, to = 10, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %>%\n  mutate(error = if_else(abs(weight - model) == 0, 0, 1)) %>%\n  group_by(model) %>%\n  summarize(error_sum = sum(error)) %>%\n  ggplot(aes(x = model, y = error_sum)) +\n    geom_line() + \n    coord_cartesian(ylim = c(0, 25)) +\n    scale_x_continuous(breaks = 1:10) +\n    labs(x = \"Model (weight = x kg)\", y = \"Error (sum of errors)\")\n\n\n\n\n\n\n\n\n\nWe see that almost all models perform poorly. The errors range from 23 to 25. Most models seem to have an error of 25, which means they do not accurately describe any of the 25 Pokémon. Some have an error of 24, meaning they describe the weight of 1 Pokémon correctly. There is 1 model with an error of 23: weight = 6.9 kg. Apparently there are 2 Pokémon with a weight of 6.9, which means that this model outperforms the others.\nDespite there being a single model that outperforms the others in this set of models, it’s still a pretty poor model. After all, it is wrong 23 out of 25 times. Perhaps there are some models that outperform this model, but it’s unlikely. That’s because we’re defining error here in a very crude way. The model needs to exactly match the weight of the Pokémon, or else it counts as an error. Saying a weight is 6 kg, while it is in fact 10 kg, is as wrong as saying the weight is 60 kg.\nInstead of defining error in this way, we can redefine it in a way that takes into account the degree of error. We can define error as the difference between the actual data point and the model’s value. So, in the case of our weight = 6 kg model, an actual weight of 10 kg would have an error of 10 - 6 = 4. This definition of error is often referred to as the residual.\nBelow I plot the residuals of the first 25 Pokémon for our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  geom_segment(aes(xend = pokedex, y = 6, yend = weight), linetype = 2) +\n  geom_point() +\n  geom_abline(intercept = 6, slope = 0) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nWe can add up all of the (absolute) residuals to determine the model’s error. Just like with the binary definition of error, we can then compare multiple models. This is what you see in the graph below. For each model, this time ranging from weight = 1 kg to weight = 100 kg, the absolute residuals were calculated and added together.\n\n\nCode\nexpand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %>%\n  mutate(error = abs(weight - model)) %>%\n  group_by(model) %>%\n  summarize(error_sum = sum(error)) %>%\n  ggplot(aes(x = model, y = error_sum)) +\n    geom_line() +\n    scale_y_continuous(breaks = seq(from = 500, to = 1900, by = 200)) +\n    labs(x = \"Model (weight = x kg)\", y = \"Error (sum of residuals)\")\n\n\n\n\n\n\n\n\n\nThis graph looks very different compared to the graph where we calculated the error defined as the sum of misses. Now we see that some kind of minimum appears. Unlike the binary definition of error, it now looks like there are fewer best models. More importantly, though, we have defined error in a less crude manner, meaning that the better models indeed capture the data much better than before.\nBut we might still not be entirely happy with this new definition of error either. Calculating the sum of absolute residuals for each model comes with another conceptual problem.\nWhen you sum the number of absolute errors, four errors of 1 are equal to a single error of 4. In other words, you could have a model that is slightly off multiple times or one that might make fewer, but larger, errors. Both would be counted as equally wrong. What do we think of that? Conceptually speaking, we might find it more problematic when a model is very wrong than when the model is slightly off multiple times. If we think that, we need another definition of error.\nTo address this issue, we can square the residuals before adding them together. That way, larger errors become relatively larger compared to smaller errors. Using our previous example, summing four residuals of 1 remains 4, but a single residual of 4 becomes 4 * 4 = 16. The model now gets punished more severely for making large mistakes.\nUsing this new definition of error, we again plot the error for each model, from 1 to 100.\n\n\nCode\nexpand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %>%\n  mutate(error = abs(weight - model)^2) %>%\n  group_by(model) %>%\n  summarize(error_sum = sum(error)) %>%\n  ggplot(aes(x = model, y = error_sum)) +\n    geom_line() +\n    geom_vline(xintercept = mean(pull(pokemon25, weight)), linetype = 2) +\n    labs(x = \"Model\", y = \"Error (sum of squared residuals)\")\n\n\n\n\n\n\n\n\n\nWe see a smooth curve, with a clear minimum indicated by the vertical dashed line. This vertical line indicates the model that best describes the data. What is the value of the best model exactly? In this case, the answer is 26.144. And it turns out, there is an easy way to determine this value."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#estimating-the-model-from-the-data",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#estimating-the-model-from-the-data",
    "title": "Understanding Regression (Part 1)",
    "section": "Estimating the model from the data",
    "text": "Estimating the model from the data\nRather than setting a specific value and seeing how it fits the data, we can also use the data to determine the value that best fits the data. In the previous graph we saw that the best fitting model is one where the weight is equal to 26.144. This value turns out to be the mean of the different weights we have observed in our sample. Had we defined error as simply the sum of absolute residuals, this would be a different value. In fact, the best fitting value would then be equal to 13, or the median. And had we used the binary definition of error, the best fitting value would be the mode, which in our case is: 6.9. The table below shows an overview of which technique can be used to find the best fitting value, depending on the error definition.\n\n\nCode\ntibble(\n  error_definition = c(\"sum of errors\", \"sum of absolute residuals\", \"sum of squared residuals\"),\n  estimation_technique = c(\"mode\", \"median\", \"mean\")\n) %>%\n  kable(col.names = c(\"Error definition\", \"Estimation technique\"), digits = 2) \n\n\n\n\n\nError definition\nEstimation technique\n\n\n\n\nsum of errors\nmode\n\n\nsum of absolute residuals\nmedian\n\n\nsum of squared residuals\nmean\n\n\n\n\n\nWe can now update our model to refer to the estimation technique, rather than a fixed value. Given that the third definition of error seems to be most suitable, both pragmatically and conceptually, we’ll use the mean:\n\nweight = mean(weight)\n\nThis is also the value you get when you perform a regression analysis in R. Using R’s lm function, we can run our model with the following code:\n\n\nCode\nlm(weight ~ 1, data = pokemon25)\n\n\n\nCall:\nlm(formula = weight ~ 1, data = pokemon25)\n\nCoefficients:\n(Intercept)  \n      26.14  \n\n\nBy regressing weight onto 1 we are telling R to run an intercept-only model. This means that R will estimate which line will best fit all the values in the outcome variable, just like we have done ourselves earlier by testing different models such as weight = 6 kg.\nR spits out the following result:\n\n\nCode\nlm(weight ~ 1, data = pokemon25)\n\n\n\nCall:\nlm(formula = weight ~ 1, data = pokemon25)\n\nCoefficients:\n(Intercept)  \n      26.14  \n\n\nThe result is an intercept value of 26.144, which matches the mean of the weights.\nSo, we now know where the intercept comes from when we run an intercept-only model. It is the mean of the data we are trying to model. Note that it is only the mean because we defined the error as the sum of squared residuals. Had we defined the error differently, such as the sum of absolute residuals or the sum of errors, the intercept would be the median or mode of the data instead. Why did we use the sum of squared residuals? We had a conceptual reason of wanting to punish larger residuals relatively more than several smaller errors. It turns out there is another reason to favor squared residuals, which has to do with a nice property of the mean vs. the median. This will be covered in Part 2 of ‘Understanding Regression’.\nThis post was last updated on 2022-04-07."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html",
    "href": "content/projects/animal-welfare/animal-welfare.html",
    "title": "Animal welfare",
    "section": "",
    "text": "This is not really a project, but rather a topic I want to focus on more in my research. I am only just starting this new focus, so there is not yet much I can show, but below I explain why I want to focus on this more and some steps I’ve taken so far."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "href": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "title": "Animal welfare",
    "section": "Why is this important?",
    "text": "Why is this important?\n\n“It may come one day to be recognized, that the number of legs, the villosity of the skin, or the termination of the os sacrum, are reasons equally insufficient for abandoning a sensitive being to the same fate. What else is it that should trace the insuperable line? Is it the faculty of reason, or perhaps, the faculty for discourse?…the question is not, Can they reason? nor, Can they talk? but, Can they suffer?\n\nThis (partial) quote by Jeremy Bentham gets to the heart of the matter. Whether something deserves moral concern is predominantly (if not only) a function of whether that something is capable of suffering. Cheating on a partner is bad not because cheating is inherently bad, but because it likely causes great suffering on the cheated-on partner. If your partner doesn’t care about being cheated on (i.e., being in an open relationship), cheating is no longer a bad thing. This shows it is about the consequences of one’s actions and whether those consequences cause suffering.\nMany animals can suffer. It is unclear and impossible with our current knowledge about consciousness to assess which animals are capable of suffering, but we know enough to confidently say that some animals can suffer. Large mammals such as cows, sheep, goats, and horses can undoubtedly suffer. Smaller creatures such as chicken and turkeys are similarly unlucky and likely also capable. It is less clear when it comes to fish, but I would put my money them being able to suffer rather than being experience-less creatures.\nThe examples of animals I used above are the kinds of animals we farm. These are the kinds of animals we treat in ways that cause them to suffer, with great intensity and in great numbers. Chicken, for example, live in crowded spaces that cause in-fighting, the spread of diseases, and deaths due to, for instance, pile ups. They are artificially selected to grow at unhealthily fast rates, causing physical abnormalities. They are prevented from displaying their instinctive behaviors, such as establishing pecking orders, dust bathing, building nests, and spreading their wings. Sometimes farmers address these problems, although not always in the animal’s best interest. Injuries due to in-fighting is reduced by cutting or burning off the beaks, thus preventing them from harming each other. Other farm animals face similar situations.\nWhat makes it worse is the scale of factory farming. In the Netherlands alone, over 600 million land animals were killed in 2019. And that’s just in the Netherlands, a pretty tiny country. In the U.S., 9.76 billion land animals were killed in 2020. These numbers are so big they almost lose their meaning. The reality is, however, that factory farming causing suffering in billions and billions of individual animals, every year.\nPeople might retort that killing animals for food is simply the natural order of things. This argument is easy to refute: The natural order also sucks. We should not look at nature to determine what is good or bad (this is called the naturalistic fallacy). In nature, all kinds of suffering takes place. Animals (including humans) die due to various causes including disease, disasters, predation, starvation, and so on. These things are normal in nature. As humans, we have done our best to remove all these natural threats from our lives because that reduces our suffering. If we want to be natural, we should invite all these threats back into our lives. Of course, that’s not what we want to do, because we don’t want to suffer.\nI think we should extent that courtesy also to wild animals. We have succeeded in making our lives a lot better, while ignoring the same problems in other species. If we care about the lives of conscious creatures (such as our fellow humans, our pets, our zoo animals), we should also care about the lives of wild animals.\nIn short, farm animals suffer in horrible ways in great numbers, and the same happens in nature (although perhaps less efficiently than in factory farms). Given that suffering is the main reason to care about something, this logically means that we should figure out ways to alleviate their suffering. I hope to contribute to this."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "href": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "title": "Animal welfare",
    "section": "What am I working on?",
    "text": "What am I working on?\nI have joined the following groups:\n\nSociety for the Psychology of Human-Animal Intergroup Relations (PHAIR)\nResearch to End Consumption of Animal Products (RECAP)\n\nBy joining these groups I hope to learn more about current research directions and to join existing projects. Eventually I also hope to use these platforms to share my own work.\nI’ve joined a project by Mercy for Animals on a multi-country survey to develop insights on people’s knowledge, attitudes, behavioral intentions and behaviors regarding farmed animal issues and key advocacy activities.\nI’ve started my own project that is a meta-analysis on meat intervention studies. There have been several meta-analyses on this topic (see here for a recent one), but I think I can contribute in some unique ways by creating a ‘live meta-analysis’ that can continuously be updated with new studies.\nI have also started a few smaller projects together with one of my students, in which we focus on people’s attitudes towards human interventions aimed at reducing the suffering of wild animals.\nFinally, I will join the Animal Advocacy Conference that will be held later this month (June 30th to July 2nd, 2021). See here for more information on this conference."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html",
    "href": "content/projects/statcheck/statcheck.html",
    "title": "statcheck",
    "section": "",
    "text": "Together with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "href": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "title": "statcheck",
    "section": "Why is this important?",
    "text": "Why is this important?\nSimilar to my tidystats project, our aim is to address a particular problem in statistics reporting: the reporting of incorrect statistics.\nAs has been shown by Michèle and her colleagues, statistics are often reported incorrectly (Nuijten et al. 2016). This is likely due to the fact that researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in, for example, meta-analyses. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "href": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "title": "statcheck",
    "section": "What am I working on?",
    "text": "What am I working on?\nAdmittedly, I am simply joining Michèle and her efforts to help researchers make fewer typos. She and her colleagues have already done a lot of the work–we’re now just trying to make it even better. For example, Sacha Epskamp and Michèle developed statcheck. statcheck is an R package designed to catch statistical reporting mistakes. It works by first extracting statistics from a paper (e.g., t values, degrees of freedom, p-values). It then uses the test statistic and degrees of freedom to re-calculate the p-value and compare it to the reported p-value. If the two don’t match, there is probably a reporting mistake.\nYou can use the statcheck package in R to check your paper or you can use the web app. Using the web app consists of simply uploading your paper and checking the results. You can then go back to the paper and correct the mistakes.\nWith my experience creating tidystats, and particularly the tidystats Word add-in, we’ve started to create a Word add-in for statcheck. This add-in allows researchers to scan their document for statistical inconsistencies, find them, and fix them. This add-in is currently in beta and we hope to release it soon.\nWe are also working on improving statcheck together with the eScience Center. Together with their help we hope to expand statcheck so it can catch a greater variety of statistical inconsistencies. We have had some prepatory meetings with them and plan to fully begin this project this Fall."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#links",
    "href": "content/projects/statcheck/statcheck.html#links",
    "title": "statcheck",
    "section": "Links",
    "text": "Links\n\nThe web app\nThe R package on CRAN\nThe GitHub page of statcheck\nThe GitHub page of the statcheck Word add-in."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html",
    "href": "content/projects/tidystats/tidystats.html",
    "title": "tidystats",
    "section": "",
    "text": "tidystats is a project centered around creating software to improve how statistics are reported and shared in the field of (social) psychology."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "href": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "title": "tidystats",
    "section": "Why is this important?",
    "text": "Why is this important?\nWith this project, I hope to address two problems in statistics reporting: Incorrect and incomplete statistics reporting.\nStatistics are often reported incorrectly (Nuijten et al. 2016). I think this is because researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in, for example, meta-analyses. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test.\nThere is also a more fundamental issue. Researchers usually only report the statistics in their manuscript and nowhere else. As a result, researchers face trade-offs between reporting all statistics, writing a legible text, and journal guidelines. Reporting all statistics makes results sections difficult (and boring) to read and it also takes up valuable space. Consequently, researchers are likely to only report the statistics that they deem to be relevant, rather than reporting all statistics. While this is fine for someone who wants to simply read the paper and get the main takeaway, this is not desirable from a cumulative science perspective. All statistics should be easily available so they can be build on in future research."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "href": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "title": "tidystats",
    "section": "What am I working on?",
    "text": "What am I working on?\nI have designed an R package called tidystats that enables researchers to export the statistics from their analyses into a single file. It works by adding your analyses to a list (in R) and then exporting this list to a JSON file. This file will contain all the statistics from the analyses, in an organized format, ready to be used in other software.\nBy storing the output of statistical tests into a separate file, rather than only in one’s manuscript, the researcher no longer needs to worry about which analyses to report in the space-limited manuscript. They can simply share the file together with the manuscript, on OSF or as supplemental material.\nAn additional benefit is that because JSON files are easy to read for computers, it is (relatively) easy to write software that does cool things with these files.\nAn example of software that can read the JSON file is the tidystats Microsoft Word add-in. This add-in can be installed via the Add-in Store from inside Word. With this add-in, researchers can upload the JSON file, which will produce a user-friendly list of their analyses. Clicking on an analysis reveals the associated statistics and clicking on a statistics inserts it into the document. This add-in also comes with several time-saving features, such as inserting multiple statistics at once (immediately in APA style) and automatic updating of statistics.\nRecently, I’ve also submitted a grant to expand the functionality of tidystats. Hopefully I will obtain this grant, so I can hire someone who will help me expand tidystats, both in terms of adding support for different types of analyses and by expanding to other platforms, such as Python and Google Docs.\nBesides working on the software itself, I also spent some time on making it accessible. I have given talks introducing tidystats and I’ve created a website to help people become familiar with tidystats."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#links",
    "href": "content/projects/tidystats/tidystats.html#links",
    "title": "tidystats",
    "section": "Links",
    "text": "Links\n\nThe tidystats website\nR package on CRAN\nR package GitHub repository\nThe tidystats Word add-in in AppSource (the Office add-in store)\nWord add-in GitHub repository\nA blog post describing an example of using tidystats"
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "title": "Cognitive dissonance",
    "section": "",
    "text": "Cognitive dissonance refers to a state of aversive arousal that is experienced when people realize they possess mutually inconsistent cognitions. This state is the foundation of cognitive dissonance theory (CDT)–a theory developed by Leon Festinger in 1957. Several of my projects are aimed at assessing the evidence for this theory and at applying this theory to other issues."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "title": "Cognitive dissonance",
    "section": "Why is this important?",
    "text": "Why is this important?\nThe theory of cognitive dissonance can explain many different phenomena that we should understand so that we may intervene and improve the lives of others. For example, cognitive dissonance theory has been used to explain religious beliefs, unhealthy behaviors, and people’s attitude towards animals.\nBefore the theory can be applied, however, it needs to be verified. We need to have sufficient evidence to believe in the theory. The social psychological evidence we have for the theory is, however, quite weak. The research stems from old research, mostly conducted in the 50s, 60s, and 70s. While this would not necessarily be a problem, it is a problem in the case of social psychology. The original studies were conducted with extremely low sample sizes and without pre-registration, or other tools that limit p-hacking. This means that many past findings may be false positives, which is supported by recent findings that show many findings in psychology do not replicate."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "title": "Cognitive dissonance",
    "section": "What am I working on?",
    "text": "What am I working on?\nI am one of the lead investigators of a large-scaled replication project. In this project, we will try and replicate a seminal finding in the cognitive dissonance literature. Specifically, our aim is to replicate the classic finding that people who write a counterattitudinal essay (e.g., students arguing in favor of a tuition increase) become more in favor of the position they argued for. We have submitted this project as a registered report to Advanced in Methods and Practices in Psychological Science (AMPPS). There it has received an in-principle acceptance. Data collection is expected to begin at the start of the new academic year.\nI also hope to start up a meta-analysis project to produce live reviews of studies from the cognitive dissonance literature."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "title": "Cognitive dissonance",
    "section": "Links",
    "text": "Links\n\nThe landing page of our large-scaled replication project\nThe stage-1 accepted manuscript"
  },
  {
    "objectID": "content/projects.html",
    "href": "content/projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is not really a project, but rather a topic I want to focus on more in my research. I am only just starting this new focus, so there is not yet much I can show, but…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance refers to a state of aversive arousal that is experienced when people realize they possess mutually inconsistent cognitions. This state is the…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidystats is a project centered around creating software to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "Before joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University.\nOn this website you can find information about some of the projects I’m involved in. Some notable projects I’m working on are tidystats and a large-scaled replication study of cognitive dissonance. Besides writing about these projects, I also blog posts about various topics, including tutorials or opinion pieces.\nOne of my main research interests concern animal welfare. I think animal welfare, and their lack thereof, is one of the most pressing issues in the world at this moment and as a fruitful area of research where influential theories in social psychology (such as cognitive dissonance) can be applied and tested.\nI’m also interested in the methodology of psychological research and ways to improve how we conduct science. A notable project I’m working on is tidystats. This is a software solution to help researchers more easily and more reproducibly report statistics in scientific manuscripts. It’s main goal is to get researchers to report more statistics with fewer errors. I’m pretty proud of this project, so please check it out on the tidystats project or the tidystats website.\nI also have teaching experience thanks to my time as an assistant professor. I’m quite experienced in teaching undergraduate courses, in both small and large groups of students. Besides course work I have also provided many R workshops (although I have less time for that now).\nThis website is created using Quartro."
  },
  {
    "objectID": "content/cv/cv.html",
    "href": "content/cv/cv.html",
    "title": "CV",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\n\n\n\n\n\nE-mail\n\n\nGoogle Scholar\n\n\nGitHub\n\n\nTwitter\n\n\n\n\nEmployment\n\n\n\n2021-current\n\n\nSenior Behavioral Scientist at Rethink Priorities\n\n\n\n\n2018-2021\n\n\nTenure track Assistant Professor in social psychology at Tilburg University\n\n\n\n\n2016-2018\n\n\nFixed term Assistant Professor in social psychology at Tilburg University\n\n\n\n\n2012-2016\n\n\nGraduate student on the topic of physiological arousal in meaning maintenance at Tilburg University\n\n\n\n\n2011-2012\n\n\nStudent assistant during my Research Master: Behavioral Science at Nijmegen University\n\n\n\n\n2012-2013\n\n\nProgrammer of experimental psychology tasks for FrieslandCampina\n\n\n\n\n2007-2010\n\n\nMedia analyst for Report International\n\n\n\n\n\nEducation\n\n\n\n2012-2016\n\n\nGraduate student at Tilburg University supervised by prof. dr. Ilja van Beest and dr. Travis Proulx\n\n\n\n\n2012-2016\n\n\nStudent member of the Kurt Lewin Institute (KLI)\n\n\n\n\n2010-2012\n\n\nResearch Master Behavioural Science at Nijmegen University\n\n\n\n\n2010-2012\n\n\nExpert track in data-analysis\n\n\n\n\n2007-2010\n\n\nPsychology BSc. at Nijmegen University\n\n\n\n\n2007-2010\n\n\nHonours Program of Psychology at Nijmegen University\n\n\n\n\n\nPublications\n\n\nStage-1 accepted manuscripts\n\n\n\nSleegers, W. W. A., Vaidis, D. (shared first author) et al. (2021). A multi-lab replication of the induced compliance paradigm of cognitive dissonance. Advances in Methods and Practices in Psychological Science. https://osf.io/52wpj\n\n\n\n\n\nIn preparation\n\n\n\nvan Leeuwen, F., Jaeger, B., Sleegers, W. W. A., & Petersen, M. B. (in prep.) Pathogen avoidance and conformity: Salient infectious disease does not increase conformity.\n\n\n\n\nJaeger, B., Sleegers, W. W. A., Stern, J., Penke, L., & Jones, A. (in prep.) The accuracy and meta-accuracy of personality impressions from faces.\n\n\n\n\nSleegers, W. W. A. & Jaeger, B. (in prep.) The social cost of correcting others.\n\n\n\n\n\nPreprints\n\n\n\nJaeger, B., Sleegers, W. W. A. (2020) Racial discrimination in the sharing economy: Evidence from Airbnb markets across the world. https://psyarxiv.com/qusxf\n\n\n\n\nBreznau, N., et al. (2021) The Crowdsourced Replication Initiative: Investigating Immigration and Social Policy Preferences. Executive Report. https://osf.io/preprints/socarxiv/6j9qb/\n\n\n\n\nBreznau, N., et al. (2021) How many replicators does it take to achieve reliability? Investigating researcher variability in a crowdsourced replication. https://osf.io/preprints/socarxiv/j7qta/\n\n\n\n\n\nPeer-reviewed journals\n\n\n\nPronk, T. M., Bogaers, R. I., Verheijen, M. S., Sleegers, W. W. A. (in press). Pupil size predicts partner choices in online dating. Social Cognition.\n\n\n\n\nEvans, A. M., Kogler, C., & Sleegers, W. W. A. (in press). No effect of synchronicity in online social dilemma experiments: A registered report. Judgment and Decision Making.\n\n\n\n\nVan Osch, Y., & Sleegers, W. W. A. (2021). Replicating and reversing the group attractiveness effect: Relatively unattractive groups are perceived as less attractive than the average attractiveness of their members. Acta Psychologica, 217, 103331. https://.doi.org/10.1016/j.actpsy.2021.103331\n\n\n\n\nBrandt, M., Sleegers, W. W. A. (2021) Evaluating belief system networks as a theory of political belief system dynamics, 25(2), 159-185. https://doi.org/10.1177/1088868321993751\n\n\n\n\nJones, B. C., DeBruine, L. M., Flake, J. K., et al. (2021) To which world regions does the valence–dominance model of social perception apply? Nature Human Behavior, 5, 159–169. https://doi.org/10.1038/s41562-020-01007-2\n\n\n\n\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2020) Pupillometry and hindsight bias: Physiological arousal predicts compensatory behavior. Social Psychological and Personality Science. https://doi.org/10.1177/1948550620966153\n\n\n\n\nEvans, A., Sleegers, W. W. A., & Mlakar, Ž. (2020). Individual differences in receptivity to scientific bullshit. Judgment and Decision Making, 15(3), 401-412.\n\n\n\n\nJaeger, B., Sleegers, W. W. A., & Evans, A. M. (2020). Automated classification of demographics from face images: A tutorial and validation. Social and Personality Psychology Compass, 14(3). https://doi.org/10.1111/spc3.12520\n\n\n\n\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2019). Confirmation bias and misconceptions: Pupillometric evidence for a confirmation bias in misconceptions feedback. Biological Psychology, 145, 76–83. https://doi.org/10.1016/j.biopsycho.2019.03.018\n\n\n\n\nBender, M., van Osch, Y., Sleegers, W. W. A., & Ye, M. (2019). Social support benefits psychological adjustment of international students: Evidence from a meta-analysis. Journal of Cross-Cultural Psychology, 50(7), 827–847. https://doi.org/10.1177/0022022119861151\n\n\n\n\nVan ’t Veer, A. E., & Sleegers, W. W. A. (2019). Psychology data from an exploration of the effect of anticipatory stress on disgust vs. Non-disgust related moral judgments. Journal of Open Psychology Data, 7(1), 1. https://doi.org/10.5334/jopd.43\n\n\n\n\nJaeger, B., Sleegers, W. W. A., Evans, A. M., Stel, M., & van Beest, I. (2018). The effects of facial attractiveness and trustworthiness in online peer-to-peer markets. Journal of Economic Psychology. https://doi.org/https://doi.org/10.1016/j.joep.2018.11.004\n\n\n\n\nProulx, T., Sleegers, W. W. A., & Tritt, S. (2017). The expectancy bias: Expectancy-violating faces evoke earlier pupillary dilation than neutral or negative faces. Journal of Experimental Social Psychology, 70, 69-79. https://doi.org/10.1016/j.jesp.2016.12.003\n\n\n\n\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2017). The social pain of Cyberball: Decreased pupillary reactivity to exclusion cues. Journal of Experimental Social Psychology, 69, 187–200. https://doi.org/10.1016/j.jesp.2016.08.004\n\n\n\n\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2015). Extremism reduces conflict arousal and increases values affirmation in response to meaning violations. Biological Psychology, 108, 126–131. https://doi.org/10.1016/j.biopsycho.2015.03.012\n\n\n\n\nSleegers, W. W. A., & Proulx, T. (2015). The comfort of approach: Self-soothing effects of behavioral approach in response to meaning violations. Frontiers in Psychology, 5, 1–10. https://doi.org/10.3389/fpsyg.2014.01568\n\n\n\n\n\nBook chapters\n\n\n\nvan Beest, I., & Sleegers, W.W.A (2019). Physiostracism: A case for non-invasive measures of arousal in ostracism research. In S. C. Rudert, R. Greifeneder, & K. D. Williams (Eds.), Current directions in ostracism, social exclusion and rejectionresearch. Routledge. https://doi.org/10.4324/9781351255912\n\n\n\n\n\nDissertation\n\n\n\nSleegers, W. W. A. (2017). Meaning and pupillometry: The role of physiological arousal in meaning maintenance (Doctoral dissertation). Retrieved from https://pure.uvt.nl/portal/en/publications/meaning-and-pupillometry(20680e63-e785-43d0-a3ae-e97b26de5f05).html\n\n\n\n\n\nSoftware\n\n\n\nSleegers, W. W. A. (2020). tidystats: Save output of statistical tests (Version 0.5) [Computer software]. https://doi.org/10.5281/zenodo.4041859\n\n\n\n\nSleegers, W. W. A. (2020). tidystats (Version 1) [Computer software]. https://doi.org/10.5281/zenodo.4434634\n\n\n\n\n\nWebsites\n\n\n\nMy personal website where I blog about (some of) my research.https://www.willemsleegers.com\n\n\n\n\nThe tidystats website, a support website for my tidystats software.https://www.tidystats.io\n\n\n\n\n\nPresentations\n\n\nInvited talks\n\n\n\nSleegers, W. W. A. (2021, March). tidystats. Talk for a research group at the Ministry of Defence.\n\n\n\n\nSleegers, W. W. A. (2021, February). tidystats. Talk for the BSI at Nijmegen University.\n\n\n\n\nSleegers, W. W. A. (2021, February). Cognitive dissonance RRR. Lab meeting at Cardiff University.\n\n\n\n\nSleegers, W. W. A. (2018, December). Pupillometry and psychology. Colloquium presentation for the Laboratoire de Psychologie Sociale department at Paris Descartes University.\n\n\n\n\nSleegers, W. W. A. (2018, October) tidystats. Colloquium presentation for the Methodology and Statistics department at Leiden University.\n\n\n\n\nSleegers, W. W. A. (2018, March) tidystats. Colloquium presentation for the MTO department at Tilburg University.\n\n\n\n\nSleegers, W. W. A. (2017, December). Meaning and pupillometry: The role of physiological arousal in meaning maintenance. Presentation at the ASPO conference as part of receiving the best ASPO dissertation award.\n\n\n\n\nSleegers, W. W. A. (2017, March). Pupillometry and psychological processes. Colloquium presentation at Cardiff University.\n\n\n\n\nSleegers, W. W. A., Proulx, T. & Van Beest, I. (2015, October). Capturing the physiological response to meaning violations: An eye tracker approach. Colloquium presentation at Tilburg University.\n\n\n\n\n\nConference presentations\n\n\n\nSleegers, W. W. A. (2019, July) tidystats. Lightning talk at the SIPS conference, Rotterdam, the Netherlands.\n\n\n\n\nSleegers, W. W. A. & Jaeger, B. (2019, December) The Social Cost of Correcting Others. Talk at the ASPO conference, Wageningen, the Netherlands.\n\n\n\n\nSleegers, W. W. A. (2018, June) tidystats. Lightning talk at the SIPS conference, Grand Rapids, MI.\n\n\n\n\nSleegers, W. W. A. (2017, August). oTree for social scientists. Presentation at the TIBER conference, Tilburg, the Netherlands.\n\n\n\n\nSleegers, W. W. A., Proulx, T. & Van Beest (2016, December). Evidence of aversive arousal motivating compensatory behavior. Presentation at ASPO conference, Leiden, the Netherlands.\n\n\n\n\nProulx, T. & Sleegers, W. W. A. (2014, May). Meaning Maintenance Model: Towards a unified account of threat-compensation behaviors. Presentation at KLI conference, Zeist, the Netherlands.\n\n\n\n\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, December). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Presentation at ASPO 2014, Groningen, the Netherlands.\n\n\n\n\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, July). Ostracism and eye tracking. Presentation at EASP preconference on threat regulation, Amsterdam, the Netherlands.\n\n\n\n\n\nSmall meetings\n\n\n\nSleegers, W. W. A. (2019, November) Addressing Incorrect and Incomplete Statistics Reporting (with tidystats). Talk at the meta-research day at Tilburg University, the Netherlands.\n\n\n\n\n\nPoster presentations\n\n\n\nSleegers, W. W. A. (2017, July). Pupillometry and psychology: Pupillometry as an experimental tool for psychologists. Poster session presented at the Ostracism, Social Exclusion, and Rejection conference, Vitznau, Switzerland.\n\n\n\n\nSleegers, W. W. A., Proulx, T., & Van Beest (2016, January). Ostracism and eye tracking: Decreased pupillary reactivity to exclusion cues. Poster session presented at the SPSP conference, San Diego.\n\n\n\n\nSleegers, W. W. A., Proulx, T., & Van Beest (2015, December). Meaning and misconceptions: The effect of error feedback and commitment towards misconceptions on pupil size. Poster session presented at ASPO conference, Amsterdam.\n\n\n\n\nSleegers, W. W. A., Proulx, T. & Van Beest (2015, March). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Poster session presented at ICPS conference, Amsterdam, the Netherlands.\n\n\n\n\nSleegers, W. W. A., Proulx, T, & Van Beest, I. (2014, May). Extremism and the response to meaning threats: Extremism reduces pupillary response to threat and increases affirmation of values. Poster session presented at the KLI conference, Zeist, the Netherlands.\n\n\n\n\n\nValorization presentations\n\n\n\nSleegers, W. W. A., & Wagemans, F. M. A. (2017, November). The psychology behind eye tracking. Presentation organized by the Academic Forum of Tilburg University.\n\n\n\n\nWagemans, F. M. A., & Sleegers, W. W. A. (2017, June). Waar walg jij van? Presentation at Mundial festival on attentional processes and disgust sensitivity using eye tracking.\n\n\n\n\n\nJournals\n\n\nReviewer\n\n\n\nBehavioural Processes\n\n\n\n\nBiological Psychology\n\n\n\n\nBritish Journal of Psychology\n\n\n\n\nBritish Journal of Social Psychology\n\n\n\n\nCollabra\n\n\n\n\nEuropean Journal of Social Psychology\n\n\n\n\nGroup Processes & Intergroup Relations\n\n\n\n\nInternational Journal of Psychology\n\n\n\n\nInternational Review of Social Psychology\n\n\n\n\nJournal of Consumer Behaviour\n\n\n\n\nJournal of Experimental Social Psychology\n\n\n\n\nJournal of Social and Personal Relationships\n\n\n\n\nPersonality and Social Psychology Bulletin\n\n\n\n\nPLOS ONE\n\n\n\n\nSelf and Identity\n\n\n\n\nSocial Cognition\n\n\n\n\nSocial Influence\n\n\n\n\nSocial Psychology\n\n\n\n\nCurrent Psychology\n\n\n\n\nJournal of Cognitive Psychology\n\n\n\n\n\nAwards\n\n\n\nAwards are stupid.\n\n\n\n\n\nTeaching\n\n\nCourses\n\n\n\n2017-2021\n\n\nSocial Psychology\n\n\n\n\n2016-2021\n\n\nAttitudes and Advertising\n\n\n\n\n2019-2021\n\n\nUnderstanding Data with R\n\n\n\n\n2015/2017/2019\n\n\nResearch Master: Experimental Research and Meta-Analysis\n\n\n\n\n2016-2021\n\n\nCourse in R software\n\n\n\n\n2019-2021\n\n\nUnderstanding Data with R\n\n\n\n\n\nSeminars\n\n\n\n2012-2017\n\n\nSocial Psychology\n\n\n\n\n2015-2016\n\n\nIntroduction and History of Psychology\n\n\n\n\n2014-2015\n\n\nCultural Psychology\n\n\n\n\n2013-2015\n\n\nAcademic Skills\n\n\n\n\n2012-2013\n\n\nGroup Skills\n\n\n\n\n\nIndividual lectures\n\n\n\n2016\n\n\nSocial Psychology\n\n\n\n\n2014\n\n\nIntroduction and History of Psychology on intrapersonal conflict\n\n\n\n\n2013\n\n\nIntroduction to Social Psychology for prospective students\n\n\n\n\n\nSupervision\n\n\n\n2021\n\n\nResearch Master in Psychology theses\n\n\n\n\n2016-2021\n\n\nMaster in Psychology theses\n\n\n\n\n2013-2021\n\n\nBachelor in Psychology theses\n\n\n\n\n2012-2018\n\n\nResearch Skills in Psychology\n\n\n\n\n\nCoordination\n\n\n\n2014-2021\n\n\nSocial Psychology\n\n\n\n\n2016-2021\n\n\nAttitudes and Advertising\n\n\n\n\n\nOther\n\n\n\n2013-2021\n\n\nAn introduction to R; part of the Kurt Lewin Institute course program\n\n\n\n\n\nActivities\n\n\nDepartment\n\n\n\nLab coordinator of the SP-lab\n\n\n\n\nOrganized the Voluntary Research Assistance (VRA) program\n\n\n\n\n\nFaculty\n\n\n\nMember of the Tilburg School of Social and Behavioral Sciences PhD-council from 2014 to 2016\n\n\n\n\n\nWorkshops\n\n\n\nOrganized a workshop on Bayesian Statistics for Behavioural Scientists by dr. E.J. Wagenmakers at Radboud University\n\n\n\n\nOrganized a workshop on Latent Growth Modeling using SEM by dr. Reinoud Stoel at Radboud University\n\n\n\n\n\nConferences\n\n\n\nASPO dissertation prize committee member from 2014 to 2015\n\n\n\n\n\nTechnical skills\n\n\nStatistics\n\n\n\nR: A free software environment for statistical computing and graphics\n\n\n\n\nSPSS: A proprietary data analysis program\n\n\n\n\n\nProgramming\n\n\n\nPython: A cross-platform procedural programming language\n\n\n\n\nHTML: Markup language for creating web pages and web applications\n\n\n\n\nCSS: Markup language for styling web pages and web applications\n\n\n\n\nJavaScript: A programming language for creating web applications\n\n\n\n\nDjango: A high-level Python Web framework\n\n\n\n\nHugo: A static HTML and CSS website generator\n\n\n\n\n\nExperimental design\n\n\n\nMillisecond’s Inquisit: Stimulus delivery and experimental design software\n\n\n\n\noTree: Framework based on Python and Django to create standard and interactive online psychological experiments\n\n\n\n\nTobii Studio and Tobii Studio Extensions for E-prime: software to run eye tracker experiments using Tobii eye trackers\n\n\n\n\nPsychology Software Tool’s E-Prime: Stimulus delivery and experimental design software\n\n\n\n\nAdobe’s Authorware: Stimulus delivery and experimental design software. This has been discontinued, please do not make me use it\n\n\n\n\nNeurobehavioural Systems’ Presentation®: A stimulus delivery and experimental control program for neuroscience"
  },
  {
    "objectID": "content/blog.html",
    "href": "content/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMethod sections in academic (psychology) papers usually consist of the following sections: Participants, Design, Procedure, and Materials. They also tend to be presented in this order. But is this, generally speaking, the right order? I don’t think so.\n\n\n\n\n\n\nJul 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nI illustrate how to use my tidystats software to analyze and report the results of a replication study that was part of the Many Labs 1 project.\n\n\n\n\n\n\nApr 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\n\n\nIn a recent tweet I asked the question why we use \\(n - 1\\) to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. The response consists of some statistical jargon that confuses me more, rather than less. Some of the responses were very useful, though, so I recommend checking out the replies to the tweet. Based on some of the responses I received, I will try to describe my favorite way of looking at the issue.\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this post I list papers that are useful for understanding power analyses. A curious thing happened in the field of social psychology: Social psychologists finally realized that statistical power is important. Unfortunately, they then skipped the step of figuring out how to do them correctly. Here I list some papers on power analyses that I hope help in improving the way we do them.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nregression\n\n\n\n\nThis is Part 1 of a series of blog posts on how to understand regression. The goal is to develop an intuitive understanding of the different components of regression. In this first post, we figure out where the estimate of an intercept-only regression model comes from.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willem Sleegers",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\n \n  \n   \n  twitter\n  Github"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Willem Sleegers",
    "section": "About",
    "text": "About\nI’m a Senior Behavioral Scientist at Rethink Priorities. I am part of the survey team, which means I conduct research on attitude assessments and attitude change, using surveys and experimental designs. Before joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University. On this website you can find information about some of the projects I’m involved in. I also blog posts about various topics related to my work.\nLearn more"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Willem Sleegers",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nAnimal welfare\n\n\nThis is not really a project, but rather a topic I want to focus on more in my research. I am only just starting this new focus, so there is not yet much I can show, but…\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance\n\n\nCognitive dissonance refers to a state of aversive arousal that is experienced when people realize they possess mutually inconsistent cognitions. This state is the…\n\n\n\n\n\n\n\n\n\n\nstatcheck\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer…\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\ntidystats is a project centered around creating software to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Willem Sleegers",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\n\nThe right order of Method sections\n\n\n\n\n\nMethod sections in academic (psychology) papers usually consist of the following sections: Participants, Design, Procedure, and Materials. They also tend to be presented in this order. But is this, generally speaking, the right order? I don’t think so.\n\n\n\n\n\n\nJul 8, 2021\n\n\n\n\n\n\n\n\nA tidystats example\n\n\n\n\n\nI illustrate how to use my tidystats software to analyze and report the results of a replication study that was part of the Many Labs 1 project.\n\n\n\n\n\n\nApr 25, 2021\n\n\n\n\n\n\n\n\nWhy divide by N - 1 to calculate the variance of a sample?\n\n\n\n\n\nIn a recent tweet I asked the question why we use \\(n - 1\\) to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. The response consists of some statistical jargon that confuses me more, rather than less. Some of the responses were very useful, though, so I recommend checking out the replies to the tweet. Based on some of the responses I received, I will try to describe my favorite way of looking at the issue.\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\nUseful power analysis papers\n\n\n\n\n\nIn this post I list papers that are useful for understanding power analyses. A curious thing happened in the field of social psychology: Social psychologists finally realized that statistical power is important. Unfortunately, they then skipped the step of figuring out how to do them correctly. Here I list some papers on power analyses that I hope help in improving the way we do them.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n\n\nUnderstanding Regression (Part 1)\n\n\n\n\n\nThis is Part 1 of a series of blog posts on how to understand regression. The goal is to develop an intuitive understanding of the different components of regression. In this first post, we figure out where the estimate of an intercept-only regression model comes from.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\nNo matching items\n\n\nMore posts"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Willem Sleegers",
    "section": "CV",
    "text": "CV\nI’m an Senior Behavioral Scientist at Rethink Priorities. I have recently left academia after having built up 10 years of research experience, with publications of both scientific papers in peer-reviewed journals, as well as software publications. It should be no surprise then that my skill set consists of research skills (e.g., experimental design, data analysis, writing) and technical skills (e.g., programming). I now look forward to applying my skills to topics of great impact together with my colleagues at Rethink Priorities.\nLearn more"
  }
]